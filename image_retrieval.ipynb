{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm8XKB1_oxdB",
        "outputId": "fb52c27c-de7c-414d-f2e6-766d131502fa"
      },
      "cell_type": "code",
      "source": [
        "# Additional Dependencies\n",
        "!pip install barbar torchsummary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: barbar in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install barbar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyYQg5C6-Pu-",
        "outputId": "0113b47d-c54d-4425-e880-e7c1ce91e769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting barbar\n",
            "  Downloading barbar-0.2.1-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: barbar\n",
            "Successfully installed barbar-0.2.1\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "3rU5i6LCoxdG"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import pickle\n",
        "from barbar import Bar\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import cv2\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import gc\n",
        "RANDOMSTATE = 0\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7CcN3F_oxdH",
        "outputId": "8c84214a-41c0-415e-9240-bcdc3bfddc85"
      },
      "cell_type": "code",
      "source": [
        "# Find if any accelerator is presented, if yes switch device to use CUDA or else use CPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhHFKNE7hDqV",
        "outputId": "26fb3b05-34c6-4469-ee48-a76928536c0e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1bDs7C5yoxdI",
        "outputId": "52c00793-d4c6-4646-a95d-f3ef7c2d4ad0"
      },
      "cell_type": "code",
      "source": [
        "   # preparing intermediate DataFrame\n",
        "datasetPath = Path('/content/drive/My Drive/Colab Notebooks/deeplearning/hj/Image-Retrieval12')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "df['image'] = [f for f in os.listdir(datasetPath) if os.path.isfile(os.path.join(datasetPath, f))]\n",
        "df['image'] = '/content/drive/My Drive/Colab Notebooks/deeplearning/hj/Image-Retrieval12' + df['image'].astype(str)\n",
        "\n",
        "df.head()       "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               image\n",
              "0  /content/drive/My Drive/Colab Notebooks/deeple...\n",
              "1  /content/drive/My Drive/Colab Notebooks/deeple...\n",
              "2  /content/drive/My Drive/Colab Notebooks/deeple...\n",
              "3  /content/drive/My Drive/Colab Notebooks/deeple...\n",
              "4  /content/drive/My Drive/Colab Notebooks/deeple..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e39078e-fb7e-4845-be46-8dad090d3f37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/My Drive/Colab Notebooks/deeple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/My Drive/Colab Notebooks/deeple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/My Drive/Colab Notebooks/deeple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/My Drive/Colab Notebooks/deeple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/My Drive/Colab Notebooks/deeple...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e39078e-fb7e-4845-be46-8dad090d3f37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e39078e-fb7e-4845-be46-8dad090d3f37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e39078e-fb7e-4845-be46-8dad090d3f37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "4Oq0eH4GoxdI"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "G9L1tZAEoxdJ"
      },
      "cell_type": "code",
      "source": [
        "class CBIRDataset(Dataset):\n",
        "    def __init__(self, dataFrame):\n",
        "        self.dataFrame = dataFrame\n",
        "        \n",
        "        self.transformations = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, slice):\n",
        "            raise NotImplementedError('slicing is not supported')\n",
        "        \n",
        "        row = self.dataFrame.iloc[key]\n",
        "        image = self.transformations(Image.open(row['image']))\n",
        "        return image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataFrame.index)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "12PTndcboxdJ"
      },
      "cell_type": "code",
      "source": [
        "# Intermediate Function to process data from the data retrival class\n",
        "def prepare_data(DF):\n",
        "    trainDF, validateDF = train_test_split(DF, test_size=0.15, random_state=RANDOMSTATE)\n",
        "    train_set = CBIRDataset(trainDF)\n",
        "    validate_set = CBIRDataset(validateDF)\n",
        "    \n",
        "    return train_set, validate_set"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gOK466xooxdK"
      },
      "cell_type": "markdown",
      "source": [
        "# AutoEncoder Model"
      ]
    },
    {
      "metadata": {
        "id": "RO_q_VIqoxdK"
      },
      "cell_type": "markdown",
      "source": [
        "## High Level Structure of an AutoEncoder"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xmZDvpRboxdL"
      },
      "cell_type": "code",
      "source": [
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(# in- (N,3,512,512)\n",
        "            \n",
        "            nn.Conv2d(in_channels=3, \n",
        "                      out_channels=16, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=3, \n",
        "                      padding=1),  # (32,16,171,171)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2),  # (N,16,85,85)\n",
        "            \n",
        "            nn.Conv2d(in_channels=16, \n",
        "                      out_channels=8, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=2, \n",
        "                      padding=1),  # (N,8,43,43)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=1)  # (N,8,42,42)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels = 8, \n",
        "                               out_channels=16, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=2),  # (N,16,85,85)\n",
        "            nn.ReLU(True),\n",
        " \n",
        "            nn.ConvTranspose2d(in_channels=16, \n",
        "                               out_channels=8, \n",
        "                               kernel_size=(5,5), \n",
        "                               stride=3, \n",
        "                               padding=1),  # (N,8,255,255)\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=8, \n",
        "                               out_channels=3, \n",
        "                               kernel_size=(6,6), \n",
        "                               stride=2, \n",
        "                               padding=1),  # (N,3,512,512)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Uu-Xqoy4oxdL"
      },
      "cell_type": "code",
      "source": [
        "class ConvAutoencoder_v2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder_v2, self).__init__()\n",
        "        self.encoder = nn.Sequential(# in- (N,3,512,512)\n",
        "            \n",
        "            nn.Conv2d(in_channels=3, \n",
        "                      out_channels=64, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=1, \n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=64, \n",
        "                      out_channels=64, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=1, \n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), \n",
        "            \n",
        "            nn.Conv2d(in_channels=64, \n",
        "                      out_channels=128, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=2, \n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=128, \n",
        "                      out_channels=128, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=1, \n",
        "                      padding=0), \n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), \n",
        "            \n",
        "            nn.Conv2d(in_channels=128, \n",
        "                      out_channels=256, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=2, \n",
        "                      padding=1), \n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=256, \n",
        "                      out_channels=256, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=1, \n",
        "                      padding=1), \n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=256, \n",
        "                      out_channels=256, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=1, \n",
        "                      padding=1), \n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2) \n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels = 256, \n",
        "                               out_channels=256, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=1,\n",
        "                              padding=1), \n",
        " \n",
        "            nn.ConvTranspose2d(in_channels=256, \n",
        "                               out_channels=256, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=1, \n",
        "                               padding=1),  \n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=256, \n",
        "                               out_channels=128, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=2, \n",
        "                               padding=0),  \n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels=128, \n",
        "                               out_channels=64, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=2, \n",
        "                               padding=1),  \n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(in_channels=64, \n",
        "                               out_channels=32, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=2, \n",
        "                               padding=1), \n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels=32, \n",
        "                               out_channels=32, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=2, \n",
        "                               padding=1),  \n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels=32, \n",
        "                               out_channels=3, \n",
        "                               kernel_size=(4,4), \n",
        "                               stride=2, \n",
        "                               padding=2),  \n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "wP393I3woxdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667381c4-27e2-4877-bcdd-20878a6534b0"
      },
      "cell_type": "code",
      "source": [
        "summary(ConvAutoencoder_v2().to(device),(3,512,512))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 512, 512]           1,792\n",
            "              ReLU-2         [-1, 64, 512, 512]               0\n",
            "            Conv2d-3         [-1, 64, 512, 512]          36,928\n",
            "              ReLU-4         [-1, 64, 512, 512]               0\n",
            "         MaxPool2d-5         [-1, 64, 256, 256]               0\n",
            "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
            "              ReLU-7        [-1, 128, 128, 128]               0\n",
            "            Conv2d-8        [-1, 128, 126, 126]         147,584\n",
            "              ReLU-9        [-1, 128, 126, 126]               0\n",
            "        MaxPool2d-10          [-1, 128, 63, 63]               0\n",
            "           Conv2d-11          [-1, 256, 32, 32]         295,168\n",
            "             ReLU-12          [-1, 256, 32, 32]               0\n",
            "           Conv2d-13          [-1, 256, 32, 32]         590,080\n",
            "             ReLU-14          [-1, 256, 32, 32]               0\n",
            "           Conv2d-15          [-1, 256, 32, 32]         590,080\n",
            "             ReLU-16          [-1, 256, 32, 32]               0\n",
            "        MaxPool2d-17          [-1, 256, 16, 16]               0\n",
            "  ConvTranspose2d-18          [-1, 256, 16, 16]         590,080\n",
            "  ConvTranspose2d-19          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-20          [-1, 256, 16, 16]               0\n",
            "  ConvTranspose2d-21          [-1, 128, 33, 33]         295,040\n",
            "  ConvTranspose2d-22           [-1, 64, 65, 65]          73,792\n",
            "             ReLU-23           [-1, 64, 65, 65]               0\n",
            "  ConvTranspose2d-24         [-1, 32, 129, 129]          18,464\n",
            "  ConvTranspose2d-25         [-1, 32, 257, 257]           9,248\n",
            "             ReLU-26         [-1, 32, 257, 257]               0\n",
            "  ConvTranspose2d-27          [-1, 3, 512, 512]           1,539\n",
            "             Tanh-28          [-1, 3, 512, 512]               0\n",
            "================================================================\n",
            "Total params: 3,313,731\n",
            "Trainable params: 3,313,731\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 3.00\n",
            "Forward/backward pass size (MB): 678.39\n",
            "Params size (MB): 12.64\n",
            "Estimated Total Size (MB): 694.03\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "aS8uvUe-oxdM"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Function"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Ps2-NjfUoxdM"
      },
      "cell_type": "code",
      "source": [
        "def load_ckpt(checkpoint_fpath, model, optimizer):\n",
        "    \n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    #valid_loss_min = checkpoint['valid_loss_min']\n",
        "\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
        "    print (\"=> Saving a new best\")\n",
        "    torch.save(state, filename)  # save checkpoint\n",
        "    \n",
        "def train_model(model,  \n",
        "                criterion, \n",
        "                optimizer, \n",
        "                #scheduler, \n",
        "                num_epochs):\n",
        "    since = time.time()\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = np.inf\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for idx,inputs in enumerate(Bar(dataloaders[phase])):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, inputs)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "            #if phase == 'train':\n",
        "            #    scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f}'.format(\n",
        "                phase, epoch_loss))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                save_checkpoint(state={   \n",
        "                                    'epoch': epoch,\n",
        "                                    'state_dict': model.state_dict(),\n",
        "                                    'best_loss': best_loss,\n",
        "                                    'optimizer_state_dict':optimizer.state_dict()\n",
        "                                },filename='ckpt_epoch_{}.pt'.format(epoch))\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, optimizer, epoch_loss"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zqWVumHioxdN"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "NUM_BATCHES = 32\n",
        "RETRAIN = False\n",
        "\n",
        "train_set, validate_set = prepare_data(DF=df)\n",
        "\n",
        "dataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n",
        "                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n",
        "                }\n",
        "\n",
        "dataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n",
        "\n",
        "model = ConvAutoencoder_v2().to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PmWTB5r_oxdN"
      },
      "cell_type": "code",
      "source": [
        "# If re-training is required:\n",
        "# Load the old model\n",
        "if RETRAIN == True:\n",
        "    # load the saved checkpoint\n",
        "    model, optimizer, start_epoch = load_ckpt('../input/cbirpretrained/conv_autoencoder.pt', model, optimizer)\n",
        "    print('Checkpoint Loaded')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GzNgkwlkoxdN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "8459b20c-308d-4a1e-ca35-f0cd50829410"
      },
      "cell_type": "code",
      "source": [
        "model, optimizer, loss = train_model(model=model, \n",
        "                    criterion=criterion, \n",
        "                    optimizer=optimizer, \n",
        "                    #scheduler=exp_lr_scheduler,\n",
        "                    num_epochs=EPOCHS)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-83aa324bc1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0;31m#scheduler=exp_lr_scheduler,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     num_epochs=EPOCHS)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-b8f7dbb36598>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/barbar/Bar.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-41-9681f117cfee>\", line 15, in __getitem__\n    image = self.transformations(Image.open(row['image']))\n  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2843, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/deeplearning/hj/Image-Retrieval12122.jpg'\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "oYUBJeRuoxdN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "2969ceda-99ed-47ac-df1f-ea26f0dc5a09"
      },
      "cell_type": "code",
      "source": [
        "# Save the Trained Model\n",
        "torch.save({\n",
        "            'epoch': EPOCHS,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'conv_autoencoderv2_200ep.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ecc05a243185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             }, 'conv_autoencoderv2_200ep.pt')\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_6WspRXooxdO"
      },
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ]
    },
    {
      "metadata": {
        "id": "GiWgu43coxdO"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Indexing"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EuF2IVCYoxdO"
      },
      "cell_type": "code",
      "source": [
        "transformations = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VCCQlQghoxdO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "2fca3e37-1ad1-4f88-8eae-61e14362ccf6"
      },
      "cell_type": "code",
      "source": [
        "# Load Model in Evaluation phase\n",
        "model = ConvAutoencoder_v2().to(device)\n",
        "model.load_state_dict(torch.load('../input/cbirpretrainedv2/conv_autoencoderv2_200ep.pt', map_location=device)['model_state_dict'], strict=False)\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-d6ebeedd01e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Model in Evaluation phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvAutoencoder_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/cbirpretrainedv2/conv_autoencoderv2_200ep.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/cbirpretrainedv2/conv_autoencoderv2_200ep.pt'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KesZq10WoxdO"
      },
      "cell_type": "code",
      "source": [
        "def get_latent_features(images, transformations):\n",
        "    \n",
        "    latent_features = np.zeros((4738,256,16,16))\n",
        "    #latent_features = np.zeros((4738,8,42,42))\n",
        "    \n",
        "    for i,image in enumerate(tqdm(images)):\n",
        "        tensor = transformations(Image.open(image)).to(device)\n",
        "        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n",
        "        \n",
        "    del tensor\n",
        "    gc.collect()\n",
        "    return latent_features"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "soXn6dVNoxdO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "45d4e718-9007-4ca5-e911-668c2e29b205"
      },
      "cell_type": "code",
      "source": [
        "images = df.image.values\n",
        "latent_features = get_latent_features(images, transformations)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/20090 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-f99da56a3cbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlatent_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_latent_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-11c44df8a360>\u001b[0m in \u001b[0;36mget_latent_features\u001b[0;34m(images, transformations)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mlatent_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/deeplearning/hj/Image-Retrieval._9890.jpg'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "W-WSDJugoxdP"
      },
      "cell_type": "code",
      "source": [
        "indexes = list(range(0, 4738))\n",
        "feature_dict = dict(zip(indexes,latent_features))\n",
        "index_dict = {'indexes':indexes,'features':latent_features}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KD91k2bcoxdP"
      },
      "cell_type": "code",
      "source": [
        "# write the data dictionary to disk\n",
        "#with open('features.pkl', \"wb\") as f:\n",
        "#    f.write(pickle.dumps(index_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MAGzEFlcoxdQ"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Image Retrieval "
      ]
    },
    {
      "metadata": {
        "id": "KXgRUXyzoxdQ"
      },
      "cell_type": "markdown",
      "source": [
        "<font size=\"3\"> This will be approached with two ways as discussed in the start:\n",
        "    - Euclidean Search:\n",
        "        - Identifying the Latent Features\n",
        "        - Calculating the Euclidean Distance between them\n",
        "        - Returning the closest N indexes (of images)\n",
        "    \n",
        "    - Locality Sensitive Hashing\n",
        "        - Create hashes of the feature vector from Encoder\n",
        "        - Store it in a Hashing Table\n",
        "        - Identify closest images based on hamming distance"
      ]
    },
    {
      "metadata": {
        "id": "w-QBfpUGoxdQ"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Euclidean Search Method"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4LEMEmhgoxdQ"
      },
      "cell_type": "code",
      "source": [
        "def euclidean(a, b):\n",
        "    # compute and return the euclidean distance between two vectors\n",
        "    return np.linalg.norm(a - b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "txzTyHsooxdQ"
      },
      "cell_type": "code",
      "source": [
        "def cosine_distance(a,b):\n",
        "    return scipy.spatial.distance.cosine(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "WO332JldoxdQ"
      },
      "cell_type": "code",
      "source": [
        "def perform_search(queryFeatures, index, maxResults=64):\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(index[\"features\"])):\n",
        "        # compute the euclidean distance between our query features\n",
        "        # and the features for the current image in our index, then\n",
        "        # update our results list with a 2-tuple consisting of the\n",
        "        # computed distance and the index of the image\n",
        "        d = euclidean(queryFeatures, index[\"features\"][i])\n",
        "        results.append((d, i))\n",
        "    \n",
        "    # sort the results and grab the top ones\n",
        "    results = sorted(results)[:maxResults]\n",
        "    # return the list of results\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Tc6gouqDoxdQ"
      },
      "cell_type": "code",
      "source": [
        "def build_montages(image_list, image_shape, montage_shape):\n",
        "\n",
        "    if len(image_shape) != 2:\n",
        "        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n",
        "    if len(montage_shape) != 2:\n",
        "        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n",
        "    image_montages = []\n",
        "    # start with black canvas to draw images onto\n",
        "    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n",
        "                          dtype=np.uint8)\n",
        "    cursor_pos = [0, 0]\n",
        "    start_new_img = False\n",
        "    for img in image_list:\n",
        "        if type(img).__module__ != np.__name__:\n",
        "            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n",
        "        start_new_img = False\n",
        "        img = cv2.resize(img, image_shape)\n",
        "        # draw image to black canvas\n",
        "        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n",
        "        cursor_pos[0] += image_shape[0]  # increment cursor x position\n",
        "        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n",
        "            cursor_pos[1] += image_shape[1]  # increment cursor y position\n",
        "            cursor_pos[0] = 0\n",
        "            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n",
        "                cursor_pos = [0, 0]\n",
        "                image_montages.append(montage_image)\n",
        "                # reset black canvas\n",
        "                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n",
        "                                      dtype=np.uint8)\n",
        "                start_new_img = True\n",
        "    if start_new_img is False:\n",
        "        image_montages.append(montage_image)  # add unfinished montage\n",
        "    return image_montages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "E2oVcFn1oxdR"
      },
      "cell_type": "code",
      "source": [
        "# take the features for the current image, find all similar\n",
        "# images in our dataset, and then initialize our list of result\n",
        "# images\n",
        "fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
        "queryIdx = 3166# Input Index for which images \n",
        "MAX_RESULTS = 10\n",
        "\n",
        "\n",
        "queryFeatures = latent_features[queryIdx]\n",
        "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n",
        "imgs = []\n",
        "\n",
        "# loop over the results\n",
        "for (d, j) in results:\n",
        "    img = np.array(Image.open(images[j]))\n",
        "    print(j)\n",
        "    imgs.append(img)\n",
        "\n",
        "# display the query image\n",
        "ax[0].imshow(np.array(Image.open(images[queryIdx])))\n",
        "\n",
        "# build a montage from the results and display it\n",
        "montage = build_montages(imgs, (512, 512), (5, 2))[0]\n",
        "ax[1].imshow(montage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9aGMNgf9oxdR"
      },
      "cell_type": "code",
      "source": [
        "testpath = Path('../input/testcbir/Test_Images')\n",
        "testdf = pd.DataFrame()\n",
        "\n",
        "testdf['image'] = [f for f in os.listdir(testpath) if os.path.isfile(os.path.join(testpath, f))]\n",
        "testdf['image'] = '../input/testcbir/Test_Images/' + testdf['image'].astype(str)\n",
        "\n",
        "testdf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zKgwL8J-oxdR"
      },
      "cell_type": "code",
      "source": [
        "testimages = testdf.image.values\n",
        "test_latent_features = get_latent_features(testimages, transformations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bFNeIDmDoxdR"
      },
      "cell_type": "code",
      "source": [
        "test_latent_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LI3UfybnoxdR"
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
        "MAX_RESULTS = 10\n",
        "queryIdx = 12\n",
        "\n",
        "queryFeatures = test_latent_features[queryIdx]\n",
        "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n",
        "imgs = []\n",
        "\n",
        "# loop over the results\n",
        "for (d, j) in results:\n",
        "    img = np.array(Image.open(images[j]))\n",
        "    print(j)\n",
        "    imgs.append(img)\n",
        "\n",
        "# display the query image\n",
        "ax[0].imshow(np.array(Image.open(testimages[queryIdx])))\n",
        "\n",
        "# build a montage from the results and display it\n",
        "montage = build_montages(imgs, (512, 512), (5, 2))[0]\n",
        "ax[1].imshow(montage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HIEJ21wXoxdS"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 LSHashing Method"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CezDB_BJoxdS"
      },
      "cell_type": "code",
      "source": [
        "#!pip install lshashpy3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1WprJu6ToxdS"
      },
      "cell_type": "code",
      "source": [
        "#from lshashpy3 import LSHash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3k3j69IQoxdS"
      },
      "cell_type": "code",
      "source": [
        "## Locality Sensitive Hashing\n",
        "# params\n",
        "# k = 12 # hash size\n",
        "# L = 5  # number of tables\n",
        "# d = 14112 # Dimension of Feature vector\n",
        "# lsh = LSHash(hash_size=k, input_dim=d, num_hashtables=L)\n",
        "\n",
        "# # LSH on all the images\n",
        "# for idx,vec in tqdm(feature_dict.items()):\n",
        "#     lsh.index(vec.flatten(), extra_data=idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "DT0WoImxoxdS"
      },
      "cell_type": "code",
      "source": [
        "## Exporting as pickle\n",
        "#pickle.dump(lsh, open('lsh.p', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KJolQwjGoxdS"
      },
      "cell_type": "code",
      "source": [
        "# def get_similar_item(idx, feature_dict, lsh_variable, n_items=10):\n",
        "#     response = lsh_variable.query(feature_dict[list(feature_dict.keys())[idx]].flatten(), \n",
        "#                      num_results=n_items+1, distance_func='hamming')\n",
        "    \n",
        "#     imgs = []\n",
        "#     for i in range(1, n_items+1):\n",
        "#         imgs.append(np.array(Image.open(images[response[i][0][1]])))\n",
        "#     return imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Gz5JYMkmoxdS"
      },
      "cell_type": "code",
      "source": [
        "# fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
        "# queryIdx = 5\n",
        "\n",
        "# ax[0].imshow(np.array(Image.open(images[queryIdx])))\n",
        "\n",
        "# montage = build_montages(get_similar_item(queryIdx, feature_dict, lsh,10),(512, 512), (5, 2))[0]\n",
        "# ax[1].imshow(montage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8SNzOhXdoxdT"
      },
      "cell_type": "markdown",
      "source": [
        "# Clustering of Images"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tpY3ztBnoxdT"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "NVBboC4DoxdT"
      },
      "cell_type": "code",
      "source": [
        "def get_latent_features1D(images, transformations):\n",
        "    \n",
        "    latent_features1d = []\n",
        "    \n",
        "    for i,image in enumerate(tqdm(images)):\n",
        "        tensor = transformations(Image.open(image)).to(device)\n",
        "        latent_features1d.append(model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy().flatten())\n",
        "        \n",
        "    del tensor\n",
        "    gc.collect()\n",
        "    return latent_features1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yxby2XkYoxdT"
      },
      "cell_type": "code",
      "source": [
        "images = df.image.values\n",
        "latent_features1d = get_latent_features1D(images, transformations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HpEXnLLXoxdT"
      },
      "cell_type": "code",
      "source": [
        "latent_features1d = np.array(latent_features1d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "j9S6RAY5oxdT"
      },
      "cell_type": "code",
      "source": [
        "distortions = [] \n",
        "inertias = [] \n",
        "mapping1 = {} \n",
        "mapping2 = {} \n",
        "K = range(4,10) \n",
        "  \n",
        "for k in tqdm(K): \n",
        "    #Building and fitting the model \n",
        "    kmeanModel = KMeans(n_clusters=k).fit(latent_features1d)      \n",
        "      \n",
        "    distortions.append(sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_, \n",
        "                      'euclidean'),axis=1)) / latent_features1d.shape[0]) \n",
        "    inertias.append(kmeanModel.inertia_) \n",
        "  \n",
        "    mapping1[k] = sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_, \n",
        "                 'euclidean'),axis=1)) / latent_features1d.shape[0] \n",
        "    mapping2[k] = kmeanModel.inertia_ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "cmrpLb18oxdT"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(K, distortions, 'bx-') \n",
        "plt.xlabel('Values of K') \n",
        "plt.ylabel('Distortion') \n",
        "plt.title('The Elbow Method using Distortion') \n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "sKJcGLjeoxdT"
      },
      "cell_type": "code",
      "source": [
        "X = np.array(latent_features1d)\n",
        "K = range(3,10) \n",
        "\n",
        "for n_clusters in tqdm(K):\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=RANDOMSTATE)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gUqfBCNOoxdV"
      },
      "cell_type": "markdown",
      "source": [
        "## Using SIFT/SURF/ORB technique"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "wH4lNacAoxdV"
      },
      "cell_type": "code",
      "source": [
        "def build_dictionary(xfeatures2d, images, n_clusters):\n",
        "    #print('Computing descriptors..')        \n",
        "    desc_list = []\n",
        "    \n",
        "    for image_path in images:\n",
        "        image = cv2.imread(image_path)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        kp, dsc = xfeatures2d.detectAndCompute(gray, None)\n",
        "        desc_list.extend(dsc)\n",
        "\n",
        "    desc = np.array(desc_list)\n",
        "    #print('Creating BoW dictionary using K-Means clustering with k={}..'.format(n_clusters))\n",
        "    dictionary = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, verbose=0)\n",
        "    dictionary.fit(desc)\n",
        "    \n",
        "    distortion = sum(np.min(cdist(desc, dictionary.cluster_centers_, \n",
        "                      'euclidean'),axis=1)) / desc.shape[0]\n",
        "    \n",
        "    return distortion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zwyGsdXnoxdV"
      },
      "cell_type": "code",
      "source": [
        "orb = cv2.ORB_create()\n",
        "images = df.image.values\n",
        "K = range(4,10)\n",
        "distortions = []\n",
        "\n",
        "for k in tqdm(K):\n",
        "    distortions.append(build_dictionary(orb, images, n_clusters=k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Vel6pqZfoxdV"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(K, distortions, 'bx-') \n",
        "plt.xlabel('Values of K') \n",
        "plt.ylabel('Distortion') \n",
        "plt.title('The Elbow Method using Distortion') \n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g_Cf0azRoxdV"
      },
      "cell_type": "markdown",
      "source": [
        "- The ORB technique tells us there are 6/7 major clusters that are persistent in the data"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}